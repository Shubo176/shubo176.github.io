<!doctype html>
<html lang="en" data-theme="light">
<head>
  <meta charset="utf-8">
  <title>Protein Tm Prediction — Shubo Li</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Protein melting temperature prediction via ESM and AlphaFold with biophysical features">
  <link rel="stylesheet" href="assets/style.css">
  <script defer src="assets/site.js"></script>
</head>
<body>
<header class="site-header">
  <nav class="nav">
    <a class="nav__logo" href="/">SL</a>
    <button class="nav__toggle" aria-label="Toggle navigation" onclick="toggleNav()">☰</button>
    <ul id="navmenu" class="nav__menu">
      <li><a href="/">Home</a></li>
      <li><a href="index.html#projects">Projects</a></li>
      <li><a href="mailto:ethanli041012@gmail.com">Contact</a></li>
      <li><a href="https://github.com/Shubo176" target="_blank" rel="noopener">GitHub</a></li>
      <li>
        <button id="theme-toggle" class="theme-toggle" type="button" aria-label="Toggle dark mode" onclick="toggleTheme()">
          <span aria-hidden="true">🌙</span>
          <span class="theme-toggle__label">Dark</span>
        </button>
      </li>
    </ul>
  </nav>
</header>

<main class="container">
  <nav class="breadcrumb"><a href="index.html#projects">Projects</a> / Protein Tm Prediction</nav>
  <h1>Protein Melting Temperature Prediction via ESM and AlphaFold</h1>
  <p class="muted">Proteins · Multimodal · Regression</p>

  <section class="figure">
    <h2>Workflow</h2>
    <img src="assets/img/image2.png" alt="Protein Tm project workflow">
    <figcaption>Dataset construction, embedding fusion, auxiliary head, and evaluation.</figcaption>
    <img src="assets/img/image4.png" alt="Protein Tm project workflow">
    <figcaption>OGTclassify Model Structure.</figcaption>
  </section>

  <section>
    <h2>Model Summary</h2>
    <p>
      I owned the modeling and algorithmic components of this melting-temperature project. Working with our collaborators’ curated dataset (FireProt, ProThermDB, Meltome, ThermoMutDB, and more recent literature extractions), I handled quality control after the CD-HIT deduplication stage, ensuring the 28,463 proteins we retained had consistent structural metadata and thermodynamic annotations. The dataset was stratified into a 90/10 train–validation split plus a 2023+ temporal holdout of 75 proteins used only for final generalization checks.</p>
    <p>
      My primary responsibility was to design the multimodal fusion model. I benchmarked multiple representation strategies and settled on pairing AlphaFold-derived structural embeddings (via precomputed distograms and pLDDT summaries projected to 1280 dimensions) with ESM sequence embeddings of the same width. I engineered additional handcrafted features—including Rosetta ΔΔG estimates, Shannon entropy, oligomeric state, and an OGT prior—then standardized and concatenated them into a single feature vector.</p>
    <p>
      The regression head I implemented is a three-layer MLP (Linear→ReLU→Dropout) that reduces the fused feature dimension to half-width before emitting a single Tm prediction. I experimented with residual MLP variants and attention pooling, documenting the trade-offs; the chosen architecture hit the best bias–variance balance while staying GPU-efficient for large batches. To keep the model honest about uncertainty, I added Monte Carlo dropout at inference time, which we now surface as confidence intervals when reporting predictions.</p>
    <p>
      Training infrastructure was another core deliverable. I wrapped the entire pipeline in Optuna for automated hyperparameter tuning (learning rate, hidden width H, dropout, weight decay, batch size), integrated mixed-precision training, and built a metric dashboard tracking RMSE, MAE, and Pearson correlation per fold. After tuning, we reached an RMSE of 9.37 on the temporal test set—beating prior baselines we reimplemented—and 10×100 repeated cross-validation showed stable variance.</p>
    <p>
      We also explored a classification variant that bins Tm into low/medium/high stability. I authored the dimensionality reduction stack that maps the ESM embedding from 1280→256→32 before a softmax head; this lightweight branch allows the model to serve both regression and coarse categorization depending on downstream needs.</p>
  </section>

  <p><a class="btn btn--ghost" href="index.html#projects">← Back to Projects</a></p>
</main>

<footer class="site-footer">
  <small>© <span id="year"></span> Shubo Li</small>
</footer>
</body>
</html>
